var documenterSearchIndex = {"docs":
[{"location":"examples/ex_catalog/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/ex_catalog/#","page":"-","title":"-","text":"jupyter:   jupytext:     formats: ipynb,md     textrepresentation:       extension: .md       formatname: markdown       formatversion: '1.1'       jupytextversion: 1.1.3   kernelspec:     display_name: Julia 1.1.1     language: julia     name: julia-1.1 ‚Äì-","category":"page"},{"location":"examples/ex_catalog/#","page":"-","title":"-","text":"using Plots, NPSMC","category":"page"},{"location":"examples/ex_catalog/#","page":"-","title":"-","text":"œÉ = 10.0\nœÅ = 28.0\nŒ≤ = 8.0/3","category":"page"},{"location":"examples/ex_catalog/#","page":"-","title":"-","text":"dt_integration = 0.01\ndt_states      = 1 \ndt_obs         = 8 \nparameters     = [œÉ, œÅ, Œ≤]\nvar_obs        = [1]\nnb_loop_train  = 100\nnb_loop_test   = 10\nsigma2_catalog = 0.0\nsigma2_obs     = 2.0\n\nssm = StateSpaceModel( lorenz63, \n                       dt_integration, dt_states, dt_obs, \n                       parameters, var_obs,\n                       nb_loop_train, nb_loop_test,\n                       sigma2_catalog, sigma2_obs )","category":"page"},{"location":"examples/ex_catalog/#","page":"-","title":"-","text":"xt, yo, catalog = generate_data( ssm , [10.0;0.0;0.0]);","category":"page"},{"location":"examples/ex_catalog/#","page":"-","title":"-","text":"plot(catalog.analogs[1,:])\nplot!(catalog.analogs[2,:])\nplot!(catalog.analogs[3,:])","category":"page"},{"location":"examples/ex_catalog/#","page":"-","title":"-","text":"catalog.analogs[:,1]","category":"page"},{"location":"examples/ex_catalog/#","page":"-","title":"-","text":"p = plot3d(1, xlim=(-25,25), ylim=(-25,25), zlim=(0,50),\n                title = \"Lorenz 63\", marker = 1)\nfor x in eachcol(catalog.analogs)\n    push!(p, x...)\nend\np","category":"page"},{"location":"examples/generate_data_from_function/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/generate_data_from_function/#","page":"-","title":"-","text":"jupyter:   jupytext:     formats: ipynb,jl:light     textrepresentation:       extension: .md       formatname: markdown       formatversion: '1.1'       jupytextversion: 1.1.3   kernelspec:     display_name: Julia 1.1.0     language: julia     name: julia-1.1 ‚Äì-","category":"page"},{"location":"examples/generate_data_from_function/#","page":"-","title":"-","text":"include(\"../src/models.jl\")\ninclude(\"../src/time_series.jl\")\ninclude(\"../src/state_space.jl\")\ninclude(\"../src/catalog.jl\")\ninclude(\"../src/plot.jl\")\ninclude(\"../src/generate_data.jl\")","category":"page"},{"location":"examples/generate_data_from_function/#GENERATE-SIMULATED-DATA-(SINUS-MODEL)-1","page":"-","title":"GENERATE SIMULATED DATA (SINUS MODEL)","text":"","category":"section"},{"location":"examples/generate_data_from_function/#","page":"-","title":"-","text":"# parameters\ndx       = 1                # dimension of the state\ndt_int   = 1.               # fixed integration time\ndt_model = 1                # chosen number of model time step\nvar_obs  = [0]              # indices of the observed variables\ndy       = length(var_obs)  # dimension of the observations\n\nfunction h(x) # observation model\n    dx = length(x)\n    H = Matrix(I,dx,dx) \n    H .* x \nend\njac_h(x)  = x\nconst a = 3. :: Float64\nmx(x)     = sin.(a .* x)\njac_mx(x) = a .* cos.( a .* x)\n\n# Setting covariances\nsig2_Q = 0.1\nsig2_R = 0.1\n\n# prior state\nx0 = [1.]\n\nsinus3   = SSM(h, jac_h, mx, jac_mx, dt_int, dt_model, x0, var_obs, sig2_Q, sig2_R)","category":"page"},{"location":"examples/generate_data_from_function/#","page":"-","title":"-","text":"# generate data\nT        = 2000# length of the training\nX, Y     = generate_data( sinus3, x0, T)","category":"page"},{"location":"examples/generate_data_from_function/#","page":"-","title":"-","text":"using Plots","category":"page"},{"location":"examples/generate_data_from_function/#","page":"-","title":"-","text":"values = vcat(X.u'...)\nscatter( values, mx(values))\nscatter!(values[1:end-1], values[2:end], markersize=2)","category":"page"},{"location":"examples/generate_data_from_function/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/generate_data_from_function/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/sinus_data/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/sinus_data/#","page":"-","title":"-","text":"jupyter:   jupytext:     formats: ipynb,md     textrepresentation:       extension: .md       formatname: markdown       formatversion: '1.1'       jupytextversion: 1.1.3   kernelspec:     display_name: Julia 1.1.1     language: julia     name: julia-1.1 ‚Äì-","category":"page"},{"location":"examples/sinus_data/#","page":"-","title":"-","text":"using Plots, NPSMC","category":"page"},{"location":"examples/sinus_data/#","page":"-","title":"-","text":"Œ± = 3.0\n\ndt_integration = 0.01\ndt_states      = 1 \ndt_obs         = 8 \nparams         = [Œ±]\nvar_obs        = [1]\nnb_loop_train  = 10^2 \nnb_loop_test   = 10\nsigma2_catalog = 0.0\nsigma2_obs     = 0.1\n\nssm = StateSpaceModel( sinus, \n                       dt_integration, \n                       dt_states, \n                       dt_obs, \n                       params, \n                       var_obs,\n                       nb_loop_train, \n                       nb_loop_test,\n                       sigma2_catalog, \n                       sigma2_obs )\n\nxt, yo, catalog = generate_data( ssm, [0.0] );\n","category":"page"},{"location":"examples/sinus_data/#","page":"-","title":"-","text":"plot(xt.time, vcat(xt.values...)[:,1])\nscatter!(yo.time, vcat(yo.values...)[:,1]; markersize=2)","category":"page"},{"location":"examples/sinus_data/#","page":"-","title":"-","text":"scatter(catalog.analogs[1,:], catalog.successors[1,:], markersize=1)","category":"page"},{"location":"examples/sinus_data/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/sinus_data/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/sinus_data/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/ex_time_series/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/ex_time_series/#","page":"-","title":"-","text":"jupyter:   jupytext:     formats: ipynb,md     textrepresentation:       extension: .md       formatname: markdown       formatversion: '1.1'       jupytextversion: 1.1.3   kernelspec:     display_name: Julia 1.1.1     language: julia     name: julia-1.1 ‚Äì-","category":"page"},{"location":"examples/ex_time_series/#","page":"-","title":"-","text":"using NPSMC","category":"page"},{"location":"examples/ex_time_series/#","page":"-","title":"-","text":"nt, nv = 10, 3\nxt = TimeSeries(nt, nv)","category":"page"},{"location":"examples/ex_time_series/#","page":"-","title":"-","text":"using Random\ntime = collect(0:10.0)\nvalues = [rand(nv) for i = 1:nt]\nyo = TimeSeries(time, values)","category":"page"},{"location":"examples/ex_time_series/#","page":"-","title":"-","text":"typeof(yo.t), typeof(yo.u)","category":"page"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"","category":"page"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"jupyter:   jupytext:     formats: ipynb,jl:light     textrepresentation:       extension: .md       formatname: markdown       formatversion: '1.1'       jupytextversion: 1.1.3   kernelspec:     display_name: Julia 1.1.1     language: julia     name: julia-1.1 ‚Äì-","category":"page"},{"location":"examples/lorenz96/#GENERATE-SIMULATED-DATA-(LORENZ-96-MODEL)-1","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"","category":"section"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"using Plots, DifferentialEquations, Random","category":"page"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"include(\"../src/models.jl\")\ninclude(\"../src/time_series.jl\")\ninclude(\"../src/state_space.jl\")\ninclude(\"../src/catalog.jl\")\ninclude(\"../src/generate_data.jl\")","category":"page"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"F = 8\nJ = 40 :: Int64\nparameters = [F, J]\ndt_integration = 0.05 # integration time\ndt_states = 1 # number of integration times between consecutive states (for xt and catalog)\ndt_obs = 4 # number of integration times between consecutive observations (for yo)\nvar_obs = randperm(MersenneTwister(1234), J)[1:20] # indices of the observed variables\nnb_loop_train = 100 # size of the catalog\nnb_loop_test = 10 # size of the true state and noisy observations\nsigma2_catalog = 0.   # variance of the model error to generate the catalog   \nsigma2_obs = 2. # variance of the observation error to generate observations\n\nssm = StateSpaceModel( lorenz96, \n                       dt_integration, dt_states, dt_obs, \n                       parameters, var_obs,\n                       nb_loop_train, nb_loop_test,\n                       sigma2_catalog, sigma2_obs )","category":"page"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"5 time steps (to be in the attractor space)","category":"page"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"Checks if the inegration function is well implemented.","category":"page"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"u0 = F .* ones(Float64, J)\nu0[J√∑2] = u0[J√∑2] + 0.01\n\ntspan = (0.0,5.0)\np = [F, J]\nprob  = ODEProblem(lorenz96, u0, tspan, p)\nsol = solve(prob, reltol=1e-6, saveat= dt_integration)\nx1  = [x[1] for x in sol.u]\nx20 = [x[20] for x in sol.u]\nx40 = [x[40] for x in sol.u]\nplot(sol.t, x1)\nplot!(sol.t, x20)\nplot!(sol.t, x40)","category":"page"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"Generate data and catalog","category":"page"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"# run the data generation\nxt, yo, catalog = generate_data(ssm, u0);","category":"page"},{"location":"examples/lorenz96/#PLOT-STATE,-OBSERVATIONS-AND-CATALOG-1","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"PLOT STATE, OBSERVATIONS AND CATALOG","text":"","category":"section"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"# state and observations (when available)\nplot(xt.time,  vcat(xt.values'...)[:,1], line=(:solid,:red), label=\"x1\")\nscatter!(yo.time, vcat(yo.values'...)[:,1], markersize=2)\nplot!(xt.time, vcat(xt.values'...)[:,20],line=(:solid,:blue), label=\"x20\")\nscatter!(yo.time, vcat(yo.values'...)[:,20],markersize=2)\nplot!(xt.time, vcat(xt.values'...)[:,40],line=(:solid,:green), label=\"x40\")\nscatter!(yo.time, vcat(yo.values'...)[:,40],markersize=2)\nxlabel!(\"Lorenz-96 times\")\ntitle!(\"Lorenz-96 true (continuous lines) and observed trajectories (dots)\")","category":"page"},{"location":"examples/lorenz96/#","page":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","title":"GENERATE SIMULATED DATA (LORENZ-96 MODEL)","text":"","category":"page"},{"location":"examples/ex_model_forecasting/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/ex_model_forecasting/#","page":"-","title":"-","text":"jupyter:   jupytext:     formats: ipynb,md     textrepresentation:       extension: .md       formatname: markdown       formatversion: '1.1'       jupytextversion: 1.1.3   kernelspec:     display_name: Julia 1.1.1     language: julia     name: julia-1.1 ‚Äì-","category":"page"},{"location":"examples/ex_model_forecasting/#","page":"-","title":"-","text":"using Plots\nusing NPSMC\nusing DifferentialEquations","category":"page"},{"location":"examples/ex_model_forecasting/#","page":"-","title":"-","text":"?StateSpaceModel","category":"page"},{"location":"examples/ex_model_forecasting/#","page":"-","title":"-","text":"œÉ = 10.0\nœÅ = 28.0\nŒ≤ = 8.0/3\n\ndt_integration = 0.01\ndt_states      = 1 \ndt_obs         = 8 \nparameters     = [œÉ, œÅ, Œ≤]\nvar_obs        = [1]\nnb_loop_train  = 100 \nnb_loop_test   = 10\nsigma2_catalog = 0.0\nsigma2_obs     = 2.0\n\nssm = StateSpaceModel( lorenz63,\n                       dt_integration, dt_states, dt_obs, \n                       parameters, var_obs,\n                       nb_loop_train, nb_loop_test,\n                       sigma2_catalog, sigma2_obs )\n\n# compute u0 to be in the attractor space\nu0    = [8.0;0.0;30.0]\ntspan = (0.0,5.0)\nprob  = ODEProblem(ssm.model, u0, tspan, parameters)\nu0    = last(solve(prob, reltol=1e-6, save_everystep=false))\n\nxt, yo, catalog = generate_data( ssm, u0 );","category":"page"},{"location":"examples/ex_model_forecasting/#","page":"-","title":"-","text":"plot( xt.t, vcat(xt.u'...)[:,1])\nscatter!( yo.t, vcat(yo.u'...)[:,1]; markersize=2)","category":"page"},{"location":"examples/ex_model_forecasting/#","page":"-","title":"-","text":"np = 100\ndata_assimilation = DataAssimilation( ssm, xt)\n@time xÃÇ = data_assimilation(yo, PF(np));\nRMSE(xt, xÃÇ)","category":"page"},{"location":"examples/ex_model_forecasting/#","page":"-","title":"-","text":"plot(xt.t, vcat(xÃÇ.u'...)[:,1])\nscatter!(xt.t, vcat(xt.u'...)[:,1]; markersize=2)\nplot!(xt.t, vcat(xÃÇ.u'...)[:,2])\nscatter!(xt.t, vcat(xt.u'...)[:,2]; markersize=2)\nplot!(xt.t, vcat(xÃÇ.u'...)[:,3])\nscatter!(xt.t, vcat(xt.u'...)[:,3]; markersize=2)","category":"page"},{"location":"examples/ex_model_forecasting/#","page":"-","title":"-","text":"p = plot3d(1, xlim=(-25,25), ylim=(-25,25), zlim=(0,50),\n            title = \"Lorenz 63\", marker = 2)\nfor x in eachrow(vcat(xÃÇ.u'...))\n    push!(p, x...)\nend\np","category":"page"},{"location":"examples/ex_analog_forecasting/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/ex_analog_forecasting/#","page":"-","title":"-","text":"jupyter:   jupytext:     formats: ipynb,md     textrepresentation:       extension: .md       formatname: markdown       formatversion: '1.1'       jupytextversion: 1.1.3   kernelspec:     display_name: Julia 1.1.1     language: julia     name: julia-1.1 ‚Äì-","category":"page"},{"location":"examples/ex_analog_forecasting/#","page":"-","title":"-","text":"using Plots, DifferentialEquations, NPSMC","category":"page"},{"location":"examples/ex_analog_forecasting/#","page":"-","title":"-","text":"œÉ = 10.0\nœÅ = 28.0\nŒ≤ = 8.0/3\n\ndt_integration = 0.01\ndt_states      = 1 \ndt_obs         = 8 \nvar_obs        = [1]\nnb_loop_train  = 100 \nnb_loop_test   = 10\nsigma2_catalog = 0.0\nsigma2_obs     = 2.0\n\nssm = StateSpaceModel( lorenz63,\n                       dt_integration, dt_states, dt_obs, \n                       [œÉ, œÅ, Œ≤], var_obs,\n                       nb_loop_train, nb_loop_test,\n                       sigma2_catalog, sigma2_obs )\n\n# compute u0 to be in the attractor space\nu0    = [8.0;0.0;30.0]\ntspan = (0.0,5.0)\nprob  = ODEProblem(ssm.model, u0, tspan, ssm.params)\nu0    = last(solve(prob, reltol=1e-6, save_everystep=false))\n\nxt, yo, catalog = generate_data( ssm, u0 );\ntypeof(xt)","category":"page"},{"location":"examples/ex_analog_forecasting/#","page":"-","title":"-","text":"af = AnalogForecasting( 50, xt, catalog; \n    regression = :local_linear, sampling = :multinomial )\nnp = 100\ndata_assimilation = DataAssimilation( af, xt, ssm.sigma2_obs)\nxÃÇ = data_assimilation(yo, EnKS(np));\nRMSE(xt, xÃÇ)","category":"page"},{"location":"examples/ex_analog_forecasting/#","page":"-","title":"-","text":"plot(xt.t, vcat(xt.u'...)[:,1], label=:true)\nplot!(xt.t, vcat(xÃÇ.u'...)[:,1], label=:forecasted)\nscatter!(yo.t, vcat(yo.u'...)[:,1], markersize=2, label=:observed)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"jupyter:   jupytext:     formats: ipynb,md     textrepresentation:       extension: .md       formatname: markdown       formatversion: '1.1'       jupytextversion: 1.1.3   kernelspec:     display_name: Julia 1.1.1     language: julia     name: julia-1.1 ‚Äì-","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#A-First-Look-at-the-Kalman-Filter-1","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='index-0'></a> <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#Contents-1","page":"A First Look at the Kalman Filter","title":"Contents","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"A First Look at the Kalman Filter  \nOverview  \nThe Basic Idea  \nConvergence  \nImplementation  \nExercises  \nSolutions  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#Overview-1","page":"A First Look at the Kalman Filter","title":"Overview","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"This lecture provides a simple and intuitive introduction to the Kalman filter, for those who either","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"have heard of the Kalman filter but don‚Äôt know how it works, or  \nknow the Kalman filter equations, but don‚Äôt know where they come from  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"For additional (more advanced) reading on the Kalman filter, see","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"[LS18], section 2.7.  \n[AM05]  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The second reference presents a  comprehensive treatment of the Kalman filter","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Required knowledge: Familiarity with matrix manipulations, multivariate normal distributions, covariance matrices, etc. <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#Setup-1","page":"A First Look at the Kalman Filter","title":"Setup","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"using InstantiateFromURL\nactivate_github(\"QuantEcon/QuantEconLecturePackages\", tag = \"v0.9.7\");","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"using LinearAlgebra, Statistics, Compat","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#The-Basic-Idea-1","page":"A First Look at the Kalman Filter","title":"The Basic Idea","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The Kalman filter has many applications in economics, but for now let‚Äôs pretend that we are rocket scientists","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"A missile has been launched from country Y and our mission is to track it","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Let $ x  \\in \\mathbb{R}^2 $ denote the current location of the missile‚Äîa pair indicating latitude-longitute coordinates on a map","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"At the present moment in time, the precise location $ x $ is unknown, but we do have some beliefs about $ x $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"One way to summarize our knowledge is a point prediction $ \\hat x $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"But what if the President wants to know the probability that the missile is currently over the Sea of Japan?  \nThen it is better to summarize our initial beliefs with a bivariate probability density $ p $  \n$ \\int_E p(x)dx $ indicates the probability that we attach to the missile being in region $ E $  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The density $ p $ is called our prior for the random variable $ x $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"To keep things tractable in our example,  we  assume that our prior is Gaussian. In particular, we take","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='equation-prior'></a> $ p = N(\\hat x, \\Sigma) \\tag{1} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"where $ \\hat x $ is the mean of the distribution and $ \\Sigma $ is a $ 2 \\times 2 $ covariance matrix.  In our simulations, we will suppose that","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='equation-kalman-dhxs'></a> $ \\hat x = \\left( \\begin{array}{c}     0.2 \\\n    -0.2 \\end{array}   \\right), \\qquad \\Sigma = \\left( \\begin{array}{cc}     0.4 & 0.3 \\\n    0.3 & 0.45 \\end{array}   \\right) \\tag{2} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"This density $ p(x) $ is shown below as a contour map, with the center of the red ellipse being equal to $ \\hat x $ <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"using Plots, Distributions\n\ngr(fmt = :png); # plots setup","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"# set up prior objects\nŒ£ = [0.4  0.3\n     0.3  0.45]\nxÃÇ = [0.2, -0.2]\n\n# define G and R from the equation y = Gx + N(0, R)\nG = I # this is a generic identity object that conforms to the right dimensions\nR = 0.5 .* Œ£\n\n# define A and Q\nA = [1.2  0\n     0   -0.2]\nQ = 0.3Œ£\n\ny = [2.3, -1.9]\n\n# plotting objects\nx_grid = range(-1.5, 2.9, length = 100)\ny_grid = range(-3.1, 1.7, length = 100)\n\n# generate distribution\ndist = MvNormal(xÃÇ, Œ£)\ntwo_args_to_pdf(dist) = (x, y) -> pdf(dist, [x, y]) # returns a function to be plotted\n\n# plot\ncontour(x_grid, y_grid, two_args_to_pdf(dist), fill = false,\n        color = :lighttest, cbar = false)\n#contour!(x_grid, y_grid, two_args_to_pdf(dist), fill = false, lw=1,\n#         color = :grays, cbar = false)","category":"page"},{"location":"examples/kalman/#The-Filtering-Step-1","page":"A First Look at the Kalman Filter","title":"The Filtering Step","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"We are now presented with some good news and some bad news","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The good news is that the missile has been located by our sensors, which report that the current location is $ y = (2.3, -1.9) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The next figure shows the original prior $ p(x) $ and the new reported location $ y $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"# plot the figure\nannotate!(y[1], y[2], \"üöÄ\", color = :black)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì> The bad news is that our sensors are imprecise.","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"In particular, we should interpret the output of our sensor not as $ y=x $, but rather as","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='equation-kl-measurement-model'></a> $ y = G x + v, \\quad \\text{where} \\quad v \\sim N(0, R) \\tag{3} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Here $ G $ and $ R $ are $ 2 \\times 2 $ matrices with $ R $ positive definite.  Both are assumed known, and the noise term $ v $ is assumed to be independent of $ x $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"How then should we combine our prior $ p(x) = N(\\hat x, \\Sigma) $ and this new information $ y $ to improve our understanding of the location of the missile?","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"As you may have guessed, the answer is to use Bayes‚Äô theorem, which tells us to  update our prior $ p(x) $ to $ p(x \\,|\\, y) $ via","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"$","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"p(x \\,|\\, y) = \\frac{p(y \\,|\\, x) \\, p(x)} {p(y)} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"where $ p(y) = \\int p(y \\,|\\, x) \\, p(x) dx $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"In solving for $ p(x \\,|\\, y) $, we observe that","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"$ p(x) = N(\\hat x, \\Sigma) $  \nIn view of (3), the conditional density $ p(y \\,|\\, x) $ is $ N(Gx, R) $  \n$ p(y) $ does not depend on $ x $, and enters into the calculations only as a normalizing constant  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Because we are in a linear and Gaussian framework, the updated density can be computed by calculating population linear regressions","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"In particular, the solution is known <sup><a href=#f1 id=f1-link>[1]</a></sup> to be","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"$","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"p(x \\,|\\, y) = N(\\hat x^F, \\Sigma^F) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"where","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='equation-kl-filter-exp'></a> $ \\hat x^F := \\hat x + \\Sigma G' (G \\Sigma G' + R)^{-1}(y - G \\hat x) \\quad \\text{and} \\quad \\Sigma^F := \\Sigma - \\Sigma G' (G \\Sigma G' + R)^{-1} G \\Sigma \\tag{4} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Here  $ \\Sigma G' (G \\Sigma G' + R)^{-1} $ is the matrix of population regression coefficients of the hidden object $ x - \\hat x $ on the surprise $ y - G \\hat x $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"This new density $ p(x \\,|\\, y) = N(\\hat x^F, \\Sigma^F) $ is shown in the next figure via contour lines and the color map","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The original density is left in as contour lines for comparison <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"# define posterior objects\nM = Œ£ * G' * inv(G * Œ£ * G' + R)\nxÃÇ_F = xÃÇ + M * (y - G * xÃÇ)\nŒ£_F = Œ£ - M * G * Œ£\n\n# plot the new density on the old plot\nnewdist = MvNormal(xÃÇ_F, Symmetric(Œ£_F)) # because Œ£_F\ncontour!(x_grid, y_grid, two_args_to_pdf(newdist), fill = false,\n         color = :lighttest, cbar = false)\ncontour!(x_grid, y_grid, two_args_to_pdf(newdist), fill = false, levels = 7,\n         color = :grays, cbar = false)\ncontour!(x_grid, y_grid, two_args_to_pdf(dist), fill = false, levels = 7, lw=1,\n         color = :grays, cbar = false)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì> Our new density twists the prior $ p(x) $ in a direction determined by  the new information $ y - G \\hat x $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"In generating the figure, we set $ G $ to the identity matrix and $ R = 0.5 \\Sigma $ for $ \\Sigma $ defined in (2)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='kl-forecase-step'></a> <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#The-Forecast-Step-1","page":"A First Look at the Kalman Filter","title":"The Forecast Step","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"What have we achieved so far?","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"We have obtained probabilities for the current location of the state (missile) given prior and current information","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"This is called ‚Äúfiltering‚Äù rather than forecasting, because we are filtering out noise rather than looking into the future","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"$ p(x \\,|\\, y) = N(\\hat x^F, \\Sigma^F) $ is called the filtering distribution  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"But now let‚Äôs suppose that we are given another task: to predict the location of the missile after one unit of time (whatever that may be) has elapsed","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"To do this we need a model of how the state evolves","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Let‚Äôs suppose that we have one, and that it‚Äôs linear and Gaussian. In particular,","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='equation-kl-xdynam'></a> $ x{t+1} = A xt + w{t+1}, \\quad \\text{where} \\quad wt \\sim N(0, Q) \\tag{5} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Our aim is to combine this law of motion and our current distribution $ p(x \\,|\\, y) = N(\\hat x^F, \\Sigma^F) $ to come up with a new predictive distribution for the location in one unit of time","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"In view of (5), all we have to do is introduce a random vector $ x^F \\sim N(\\hat x^F, \\Sigma^F) $ and work out the distribution of $ A x^F + w $ where $ w $ is independent of $ x^F $ and has distribution $ N(0, Q) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Since linear combinations of Gaussians are Gaussian, $ A x^F + w $ is Gaussian","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Elementary calculations and the expressions in (4) tell us that","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"$","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"\\mathbb{E} [A x^F + w] = A \\mathbb{E} x^F + \\mathbb{E} w = A \\hat x^F = A \\hat x + A \\Sigma G' (G \\Sigma G' + R)^{-1}(y - G \\hat x) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"and","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"$","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"\\operatorname{Var} [A x^F + w] = A \\operatorname{Var}[x^F] A' + Q = A \\Sigma^F A' + Q = A \\Sigma A' - A \\Sigma G' (G \\Sigma G' + R)^{-1} G \\Sigma A' + Q $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The matrix $ A \\Sigma G' (G \\Sigma G' + R)^{-1} $ is often written as $ K_{\\Sigma} $ and called the Kalman gain","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The subscript $ \\Sigma $ has been added to remind us that  $ K_{\\Sigma} $ depends on $ \\Sigma $, but not $ y $ or $ \\hat x $  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Using this notation, we can summarize our results as follows","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Our updated prediction is the density $ N(\\hat x{new}, \\Sigma{new}) $ where","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='equation-kl-mlom0'></a> $ \\begin{aligned}     \\hat x{new} &:= A \\hat x + K{\\Sigma} (y - G \\hat x) \\\n    \\Sigma{new} &:= A \\Sigma A' - K{\\Sigma} G \\Sigma A' + Q \\nonumber \\end{aligned} \\tag{6} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The density $ p{new}(x) = N(\\hat x{new}, \\Sigma_{new}) $ is called the predictive distribution  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The predictive distribution is the new density shown in the following figure, where the update has used parameters","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"$","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"A = \\left( \\begin{array}{cc}     1.2 & 0.0 \\\n    0.0 & -0.2 \\end{array}   \\right),   \\qquad Q = 0.3 * \\Sigma $ <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"# get the predictive distribution\nnew_xÃÇ = A * xÃÇ_F\nnew_Œ£ = A * Œ£_F * A' + Q\npredictdist = MvNormal(new_xÃÇ, Symmetric(new_Œ£))\n\n# plot Density 3\ncontour(x_grid, y_grid, two_args_to_pdf(predictdist), fill = false, lw = 1,\n        color = :lighttest, cbar = false)\ncontour!(x_grid, y_grid, two_args_to_pdf(dist),\n         color = :grays, cbar = false)\ncontour!(x_grid, y_grid, two_args_to_pdf(newdist), fill = false, levels = 7,\n         color = :grays, cbar = false)\nannotate!(y[1], y[2], \"\\U1F680\", color = :black)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#The-Recursive-Procedure-1","page":"A First Look at the Kalman Filter","title":"The Recursive Procedure","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='index-1'></a> Let‚Äôs look back at what we‚Äôve done","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"We started the current period with a prior $ p(x) $ for the location $ x $ of the missile","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"We then used the current measurement $ y $ to update to $ p(x \\,|\\, y) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Finally, we used the law of motion (5) for $ {xt} $ to update to $ p{new}(x) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"If we now step into the next period, we are ready to go round again, taking $ p_{new}(x) $ as the current prior","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Swapping notation $ pt(x) $ for $ p(x) $ and $ p{t+1}(x) $ for $ p_{new}(x) $, the full recursive procedure is:","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Start the current period with prior $ pt(x) = N(\\hat xt, \\Sigma_t) $  \nObserve current measurement $ y_t $  \nCompute the filtering distribution $ pt(x \\,|\\, y) = N(\\hat xt^F, \\Sigmat^F) $ from $ pt(x) $ and $ y_t $, applying Bayes rule and the conditional distribution (3)  \nCompute the predictive distribution $ p{t+1}(x) = N(\\hat x{t+1}, \\Sigma_{t+1}) $ from the filtering distribution and (5)  \nIncrement $ t $ by one and go to step 1  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Repeating (6), the dynamics for $ \\hat xt $ and $ \\Sigmat $ are as follows","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='equation-kalman-lom'></a> $ \\begin{aligned}     \\hat x{t+1} &= A \\hat xt + K{\\Sigmat} (yt - G \\hat xt) \\\n    \\Sigma{t+1} &= A \\Sigmat A' - K{\\Sigmat} G \\Sigma_t A' + Q \\nonumber \\end{aligned} \\tag{7} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"These are the standard dynamic equations for the Kalman filter (see, for example, [LS18], page 58)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='kalman-convergence'></a> <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#Convergence-1","page":"A First Look at the Kalman Filter","title":"Convergence","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The matrix $ \\Sigmat $ is a measure of the uncertainty of our prediction $ \\hat xt $ of $ x_t $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Apart from special cases, this uncertainty will never be fully resolved, regardless of how much time elapses","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"One reason is that our prediction $ \\hat x_t $ is made based on information available at $ t-1 $, not $ t $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Even if we know the precise value of $ x{t-1} $ (which we don‚Äôt), the transition equation (5) implies that $ xt = A x{t-1} + wt $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Since the shock $ wt $ is not observable at $ t-1 $, any time $ t-1 $ prediction of $ xt $ will incur some error (unless $ w_t $ is degenerate)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"However, it is certainly possible that $ \\Sigma_t $ converges to a constant matrix as $ t \\to \\infty $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"To study this topic, let‚Äôs expand the second equation in (7):","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='equation-kalman-sdy'></a> $ \\Sigma{t+1} = A \\Sigmat A' -  A \\Sigmat G' (G \\Sigmat G' + R)^{-1} G \\Sigma_t A' + Q \\tag{8} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"This is a nonlinear difference equation in $ \\Sigma_t $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"A fixed point of (8) is a constant matrix $ \\Sigma $ such that","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='equation-kalman-dare'></a> $ \\Sigma = A \\Sigma A' -  A \\Sigma G' (G \\Sigma G' + R)^{-1} G \\Sigma A' + Q \\tag{9} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Equation (8) is known as a discrete time Riccati difference equation","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Equation (9) is known as a discrete time algebraic Riccati equation","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Conditions under which a fixed point exists and the sequence $ {\\Sigma_t} $ converges to it are discussed in [AHMS96] and [AM05], chapter 4","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"A sufficient (but not necessary) condition is that all the eigenvalues $ \\lambdai $ of $ A $ satisfy $ |\\lambdai| < 1 $ (cf. e.g., [AM05], p. 77)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"(This strong condition assures that the unconditional  distribution of $ x_t $  converges as $ t \\rightarrow + \\infty $)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"In this case, for any initial choice of $ \\Sigma0 $ that is both nonnegative and symmetric, the sequence $ {\\Sigmat} $ in (8) converges to a nonnegative symmetric matrix $ \\Sigma $ that solves (9) <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#Implementation-1","page":"A First Look at the Kalman Filter","title":"Implementation","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='index-2'></a> The QuantEcon.jl package is able to implement the Kalman filter by using methods for the type Kalman","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Instance data consists of:  \nThe parameters $ A, G, Q, R $ of a given model  \nthe moments $ (\\hat xt, \\Sigmat) $ of the current prior  \nThe type Kalman from the QuantEcon.jl package has a number of methods, some that we will wait to use until we study more advanced applications in subsequent lectures  \nMethods pertinent for this lecture  are:  \nprior_to_filtered, which updates $ (\\hat xt, \\Sigmat) $ to $ (\\hat xt^F, \\Sigmat^F) $  \nfiltered_to_forecast, which updates the filtering distribution to the predictive distribution ‚Äì which becomes the new prior $ (\\hat x{t+1}, \\Sigma{t+1}) $  \nupdate, which combines the last two methods  \na stationary_values, which computes the solution to (9) and the corresponding (stationary) Kalman gain  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"You can view the program on GitHub <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#Exercises-1","page":"A First Look at the Kalman Filter","title":"Exercises","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='kalman-ex1'></a> <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#Exercise-1-1","page":"A First Look at the Kalman Filter","title":"Exercise 1","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Consider the following simple application of the Kalman filter, loosely based on [LS18], section 2.9.2","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Suppose that","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"all variables are scalars  \nthe hidden state $ {x_t} $ is in fact constant, equal to some $ \\theta \\in \\mathbb{R} $ unknown to the modeler  ","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"State dynamics are therefore given by (5) with $ A=1 $, $ Q=0 $ and $ x_0 = \\theta $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The measurement equation is $ yt = \\theta + vt $ where $ v_t $ is $ N(0,1) $ and iid","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The task of this exercise to simulate the model and, using the code from kalman.jl, plot the first five predictive densities $ pt(x) = N(\\hat xt, \\Sigma_t) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"As shown in [LS18], sections 2.9.1‚Äì2.9.2, these distributions asymptotically put all mass on the unknown value $ \\theta $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"In the simulation, take $ \\theta = 10 $, $ \\hat x0 = 8 $ and $ \\Sigma0 = 1 $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Your figure should ‚Äì modulo randomness ‚Äì look something like this","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<img src=\"https://s3-ap-southeast-2.amazonaws.com/lectures.quantecon.org/jl/static/figures/klex1_fig.png\" style=\"width:100%;height:100%\">","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='kalman-ex2'></a> <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#Exercise-2-1","page":"A First Look at the Kalman Filter","title":"Exercise 2","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The preceding figure gives some support to the idea that probability mass converges to $ \\theta $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"To get a better idea, choose a small $ \\epsilon > 0 $ and calculate","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"$","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"zt := 1 - \\int{\\theta - \\epsilon}^{\\theta + \\epsilon} p_t(x) dx $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"for $ t = 0, 1, 2, \\ldots, T $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Plot $ z_t $ against $ T $, setting $ \\epsilon = 0.1 $ and $ T = 600 $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Your figure should show error erratically declining something like this","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<img src=\"https://s3-ap-southeast-2.amazonaws.com/lectures.quantecon.org/jl/static/figures/klex2_fig.png\" style=\"width:100%;height:100%\">","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='kalman-ex3'></a> <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<!‚Äì #region ‚Äì>","category":"page"},{"location":"examples/kalman/#Exercise-3-1","page":"A First Look at the Kalman Filter","title":"Exercise 3","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"As discussed above, if the shock sequence $ {wt} $ is not degenerate, then it is not in general possible to predict $ xt $ without error at time $ t-1 $ (and this would be the case even if we could observe $ x_{t-1} $)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Let‚Äôs now compare the prediction $ \\hat xt $ made by the Kalman filter against a competitor who is allowed to observe $ x{t-1} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"This competitor will use the conditional expectation $ \\mathbb E[ xt \\,|\\, x{t-1}] $, which in this case is $ A x_{t-1} $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The conditional expectation is known to be the optimal prediction method in terms of minimizing mean squared error","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"(More precisely, the minimizer of $ \\mathbb E \\, \\| xt - g(x{t-1}) \\|^2 $ with respect to $ g $ is $ g^*(x{t-1}) := \\mathbb E[ xt \\,|\\, x_{t-1}] $)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Thus we are comparing the Kalman filter against a competitor who has more information (in the sense of being able to observe the latent state) and behaves optimally in terms of minimizing squared error","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Our horse race will be assessed in terms of squared error","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"In particular, your task is to generate a graph plotting observations of both $ \\| xt - A x{t-1} \\|^2 $ and $ \\| xt - \\hat xt \\|^2 $ against $ t $ for $ t = 1, \\ldots, 50 $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"For the parameters, set $ G = I, R = 0.5 I $ and $ Q = 0.3 I $, where $ I $ is the $ 2 \\times 2 $ identity","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Set","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"$","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"A = \\left( \\begin{array}{cc}     0.5 & 0.4 \\\n    0.6 & 0.3 \\end{array}   \\right) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"To initialize the prior density, set","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"$","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"\\Sigma_0 = \\left( \\begin{array}{cc}     0.9 & 0.3 \\\n    0.3 & 0.9 \\end{array}   \\right) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"and $ \\hat x_0 = (8, 8) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Finally, set $ x_0 = (0, 0) $","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"You should end up with a figure similar to the following (modulo randomness)","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<img src=\"https://s3-ap-southeast-2.amazonaws.com/lectures.quantecon.org/jl/static/figures/kalmanex3.png\" style=\"width:100%;height:100%\">","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Observe how, after an initial learning period, the Kalman filter performs quite well, even relative to the competitor who predicts optimally with knowledge of the latent state","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<a id='kalman-ex4'></a> <!‚Äì #endregion ‚Äì>","category":"page"},{"location":"examples/kalman/#Exercise-4-1","page":"A First Look at the Kalman Filter","title":"Exercise 4","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Try varying the coefficient $ 0.3 $ in $ Q = 0.3 I $ up and down","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Observe how the diagonal values in the stationary solution $ \\Sigma $ (see (9)) increase and decrease in line with this coefficient","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"The interpretation is that more randomness in the law of motion for $ x_t $ causes more (permanent) uncertainty in prediction","category":"page"},{"location":"examples/kalman/#Solutions-1","page":"A First Look at the Kalman Filter","title":"Solutions","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"using QuantEcon","category":"page"},{"location":"examples/kalman/#Exercise-1-2","page":"A First Look at the Kalman Filter","title":"Exercise 1","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"# parameters\nŒ∏ = 10\nA, G, Q, R = 1.0, 1.0, 0.0, 1.0\nxÃÇ_0, Œ£_0 = 8.0, 1.0\n\n# initialize Kalman filter\nkalman = Kalman(A, G, Q, R)\nset_state!(kalman, xÃÇ_0, Œ£_0)\n\nxgrid = range(Œ∏ - 5, Œ∏ + 2, length = 200)\ndensities = zeros(200, 5) # one column per round of updating\nfor i in 1:5\n    # record the current predicted mean and variance, and plot their densities\n    m, v = kalman.cur_x_hat, kalman.cur_sigma\n    densities[:, i] = pdf.(Normal(m, sqrt(v)), xgrid)\n\n    # generate the noisy signal\n    y = Œ∏ + randn()\n\n    # update the Kalman filter\n    update!(kalman, y)\nend\n\nlabels = [\"t=1\", \"t=2\", \"t=3\", \"t=4\", \"t=5\"]\nplot(xgrid, densities, label = labels, legend = :topleft, grid = false,\n     title = \"First 5 densities when theta = $Œ∏\")","category":"page"},{"location":"examples/kalman/#Exercise-2-2","page":"A First Look at the Kalman Filter","title":"Exercise 2","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"using Random, Expectations\nRandom.seed!(42)  # reproducible results\nœµ = 0.1\nkalman = Kalman(A, G, Q, R)\nset_state!(kalman, xÃÇ_0, Œ£_0)\n\nnodes, weights = qnwlege(21, Œ∏-œµ, Œ∏+œµ)\n\nT = 600\nz = zeros(T)\nfor t in 1:T\n    # record the current predicted mean and variance, and plot their densities\n    m, v = kalman.cur_x_hat, kalman.cur_sigma\n    dist = Normal(m, sqrt(v))\n    E = expectation(dist, nodes)\n    integral = E(x -> 1) # just take the pdf integral\n    z[t] = 1. - integral\n# generate the noisy signal and update the Kalman filter\nupdate!(kalman, Œ∏ + randn())\nend\n\nplot(1:T, z, fillrange = 0, color = :blue, fillalpha = 0.2, grid = false,xlims=(0, T),\n     legend = false)","category":"page"},{"location":"examples/kalman/#Exercise-3-2","page":"A First Look at the Kalman Filter","title":"Exercise 3","text":"","category":"section"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"# define A, Q, G, R\nG = I + zeros(2, 2)\nR = 0.5 .* G\nA = [0.5 0.4\n        0.6 0.3]\nQ = 0.3 .* G\n\n# define the prior density\nŒ£ = [0.9 0.3\n        0.3 0.9]\nxÃÇ = [8, 8]\n\n# initialize the Kalman filter\nkn = Kalman(A, G, Q, R)\nset_state!(kn, xÃÇ, Œ£)\n\n# set the true initial value of the state\nx = zeros(2)\n\n# print eigenvalues of A\nprintln(\"Eigenvalues of A:\\n$(eigvals(A))\")\n\n# print stationary Œ£\nS, K = stationary_values(kn)\nprintln(\"Stationary prediction error variance:\\n$S\")\n\n# generate the plot\nT = 50\ne1 = zeros(T)\ne2 = similar(e1)\nfor t in 1:T\n\n    # generate signal and update prediction\n    dist = MultivariateNormal(G * x, R)\n    y = rand(dist)\n    update!(kn, y)\n\n    # update state and record error\n    Ax = A * x\n    x = rand(MultivariateNormal(Ax, Q))\n    e1[t] = sum((a - b)^2 for (a, b) in zip(x, kn.cur_x_hat))\n    e2[t] = sum((a - b)^2 for (a, b) in zip(x, Ax))\nend\n\nplot(1:T, e1, color = :black, linewidth = 2, alpha = 0.6, label = \"Kalman filter error\",\n     grid = false)\nplot!(1:T, e2, color = :green, linewidth = 2, alpha = 0.6,\n      label = \"conditional expectation error\")","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"Footnotes","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"<p><a id=f1 href=#f1-link><strong>[1]</strong></a> See, for example, page 93 of [Bis06]. To get from his expressions to the ones used above, you will also need to apply the Woodbury matrix identity.","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"println(\"\\U1F680\")","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"","category":"page"},{"location":"examples/kalman/#","page":"A First Look at the Kalman Filter","title":"A First Look at the Kalman Filter","text":"","category":"page"},{"location":"examples/test_monte_carlo/#","page":"-","title":"-","text":"","category":"page"},{"location":"examples/test_monte_carlo/#","page":"-","title":"-","text":"jupyter:   jupytext:     formats: ipynb,jl:light     textrepresentation:       extension: .md       formatname: markdown       formatversion: '1.1'       jupytextversion: 1.1.3   kernelspec:     display_name: Julia 1.1.1     language: julia     name: julia-1.1 ‚Äì-","category":"page"},{"location":"examples/test_monte_carlo/#","page":"-","title":"-","text":"using DifferentialEquations, Distributions","category":"page"},{"location":"examples/test_monte_carlo/#","page":"-","title":"-","text":"using NPSMC","category":"page"},{"location":"examples/test_monte_carlo/#","page":"-","title":"-","text":"x0 = [8.0;0.0;30.0]\ntspan = (0.0,5.0)\np = [10.0,28.0,8/3]\nprob = ODEProblem(lorenz63, x0, tspan, p)\nx0 = last(solve(prob, reltol=1e-6, save_everystep=false))'\nx0","category":"page"},{"location":"examples/test_monte_carlo/#","page":"-","title":"-","text":"using LinearAlgebra\nnp = 10\nŒº = [0.,0.,0.]\nœÉ = 1.0 .* Matrix(I, 3, 3)\nd = MvNormal( Œº, œÉ)\nx = x0 .+ rand(d, np)'","category":"page"},{"location":"examples/test_monte_carlo/#","page":"-","title":"-","text":"dt = 1.0\ntspan = (0.0, dt)\nx0    = [8.0;0.0;30.0]\n\nprob  = ODEProblem( lorenz63, x0, tspan, p)\n\nfunction prob_func( prob, i, repeat)\n    prob.u0 .= x[i,:]\n    prob\nend\n\nmonte_prob = MonteCarloProblem(prob, prob_func=prob_func)\n\nsim = solve(monte_prob, Tsit5(), num_monte=np)","category":"page"},{"location":"examples/test_monte_carlo/#","page":"-","title":"-","text":"using Plots\n\nplot(sim)","category":"page"},{"location":"examples/test_monte_carlo/#","page":"-","title":"-","text":"xf = [last(sim[i].u) for i in 1:np]","category":"page"},{"location":"examples/test_monte_carlo/#","page":"-","title":"-","text":"","category":"page"},{"location":"#NPSMC.jl-1","page":"Home","title":"NPSMC.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Documentation for NPSMC.jl","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Modules = [NPSMC]\nOrder   = [:type, :function]","category":"page"},{"location":"#NPSMC.AnalogForecasting","page":"Home","title":"NPSMC.AnalogForecasting","text":"AnalogForecasting(k, xt, catalog)\n\nparameters of the analog forecasting method\n\nk            : number of analogs\nneighborhood : global analogs\ncatalog      : catalog with analogs and successors\nregression   : (:locallyconstant, :increment, :locallinear)\nsampling     : (:gaussian, :multinomial)\n\n\n\n\n\n","category":"type"},{"location":"#NPSMC.AnalogForecasting-Tuple{Array{Float64,2}}","page":"Home","title":"NPSMC.AnalogForecasting","text":"Apply the analog method on catalog of historical data \nto generate forecasts.\n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.DataAssimilation","page":"Home","title":"NPSMC.DataAssimilation","text":"DataAssimilation( forecasting, method, np, xt, sigma2)\n\nparameters of the filtering method\n\nmethod :chosen method (:AnEnKF, :AnEnKS, :AnPF)\nN      : number of members (AnEnKF/AnEnKS) or particles (AnPF)\n\n\n\n\n\n","category":"type"},{"location":"#NPSMC.DataAssimilation-Tuple{TimeSeries,EnKF}","page":"Home","title":"NPSMC.DataAssimilation","text":"data_assimilation( yo, da)\n\nApply stochastic and sequential data assimilation technics using  model forecasting or analog forecasting. \n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.DataAssimilation-Tuple{TimeSeries,EnKS}","page":"Home","title":"NPSMC.DataAssimilation","text":"data_assimilation( yo, da)\n\nApply stochastic and sequential data assimilation technics using  model forecasting or analog forecasting. \n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.DataAssimilation-Tuple{TimeSeries,PF}","page":"Home","title":"NPSMC.DataAssimilation","text":"data_assimilation( yo, da, PF(100) )\n\nApply particle filters data assimilation technics using  model forecasting or analog forecasting. \n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.EnKF","page":"Home","title":"NPSMC.EnKF","text":"Ensemble Kalman Filters\n\n\n\n\n\n","category":"type"},{"location":"#NPSMC.StateSpaceModel","page":"Home","title":"NPSMC.StateSpaceModel","text":"Space-State model is defined through the following equations\n\nleft\nbeginarrayl\nX_t = m(X_t-1) + eta_t \nY_t = H(X_t) + varepsilon_t\nendarray\nright\n\nX : hidden variables\nY : observed variables\ndt_integrationis the numerical time step used to solve the ODE.\ndt_states is the number of dt_integration between X_t-1 and X_t.\ndt_obs is the number of dt_integration between Y_t-1 and Y_t.\n\n\n\n\n\n","category":"type"},{"location":"#NPSMC.StateSpaceModel-Tuple{Array{Float64,2}}","page":"Home","title":"NPSMC.StateSpaceModel","text":"Apply the dynamical models to generate numerical forecasts.\n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.RMSE-Tuple{Any,Any}","page":"Home","title":"NPSMC.RMSE","text":"Compute the Root Mean Square Error between 2 n-dimensional vectors. \n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.generate_data","page":"Home","title":"NPSMC.generate_data","text":"from StateSpace generate:\n\ntrue state (xt)\npartial/noisy observations (yo)\ncatalog\n\n\n\n\n\n","category":"function"},{"location":"#NPSMC.generate_data-Tuple{NPSMC.SSM,Array{Float64,1},Int64}","page":"Home","title":"NPSMC.generate_data","text":" generate_data(ssm, T_burnin, T; seed = 1)\n\nGenerate simulated data from Space State Model\n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.lorenz63-NTuple{4,Any}","page":"Home","title":"NPSMC.lorenz63","text":"lorenz63(du, u, p, t)\n\nLorenz-63 dynamical model \n\nbegineqnarray\nu‚ÇÅ(t)  =  p‚ÇÅ ( u‚ÇÇ(t) - u‚ÇÅ(t)) \nu‚ÇÇ(t)  =  u‚ÇÅ ( p‚ÇÇ - u‚ÇÉ(t)) - u‚ÇÇ(t) \nu‚ÇÉ(t)  =  u‚ÇÇ(t)u‚ÇÅ(t) - p‚ÇÉu‚ÇÉ(t)\nendeqnarray\n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.lorenz96-NTuple{4,Any}","page":"Home","title":"NPSMC.lorenz96","text":"lorenz96(S, t, F, J)\n\nLorenz-96 dynamical model \n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.sinus-NTuple{4,Any}","page":"Home","title":"NPSMC.sinus","text":"sinus(du, u, p, t)\n\nSinus toy dynamical model \n\nu‚ÇÅ = p‚ÇÅ cos(p‚ÇÅt) \n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.SSM","page":"Home","title":"NPSMC.SSM","text":"Generate simulated data from Space State Model\n\nvar_obs            : indices of the observed variables\ndy                 : dimension of the observations\nQ                  : model covariance\nR                  : observation covariance\ndx                 : dimension of the state\ndt_int             : fixed integration time\ndt_model           : chosen number of model time step \nvar_obs            : indices of the observed variables\ndy                 : dimension of the observations\nH                  : first and third variables are observed\nh                  : observation model\njacH               : Jacobian of the observation model(for EKS_EM only)\nQ                  : model covariance\nR                  : observation covariance\n\n\n\n\n\n","category":"type"},{"location":"#NPSMC.ensure_pos_sym-Union{Tuple{Array{T,2}}, Tuple{T}} where T<:AbstractFloat","page":"Home","title":"NPSMC.ensure_pos_sym","text":"ensure_pos_sym(M::Matrix{T}; œµ::T = 1e-8) where T <: AbstractFloat\n\nEnsure that matrix M is positive and symmetric to avoid numerical errors when numbers are small by doing (M + M')/2 + œµ*I\n\nreference : StateSpaceModels.jl\n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.inv_using_SVD-Tuple{Any,Any}","page":"Home","title":"NPSMC.inv_using_SVD","text":"inv_using_SVD(Mat, eigvalMax)\n\nSVD decomposition of Matrix. \n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.mk_stochastic!-Tuple{Array{Float64,2}}","page":"Home","title":"NPSMC.mk_stochastic!","text":"Ensure the matrix is stochastic, i.e.,  the sum over the last dimension is 1.\n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.normalise!-Tuple{Any}","page":"Home","title":"NPSMC.normalise!","text":"Normalize the entries of a multidimensional array sum to 1. \n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.resample!-Tuple{Array{Int64,1},Array{Float64,1}}","page":"Home","title":"NPSMC.resample!","text":"Multinomial resampler. \n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.resample_multinomial-Tuple{Array{Float64,1}}","page":"Home","title":"NPSMC.resample_multinomial","text":"Multinomial resampler. \n\n\n\n\n\n","category":"method"},{"location":"#NPSMC.sample_discrete-Tuple{Any,Any,Any}","page":"Home","title":"NPSMC.sample_discrete","text":"Sampling from a non-uniform distribution. \n\n\n\n\n\n","category":"method"},{"location":"example/#","page":"Example","title":"Example","text":"using Plots\nusing NPSMC\nusing DifferentialEquations","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"?StateSpaceModel","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"œÉ = 10.0\nœÅ = 28.0\nŒ≤ = 8.0/3\n\ndt_integration = 0.01\ndt_states      = 1 \ndt_obs         = 8 \nparameters     = [œÉ, œÅ, Œ≤]\nvar_obs        = [1]\nnb_loop_train  = 100 \nnb_loop_test   = 10\nsigma2_catalog = 0.0\nsigma2_obs     = 2.0\n\nssm = StateSpaceModel( lorenz63,\n                       dt_integration, dt_states, dt_obs, \n                       parameters, var_obs,\n                       nb_loop_train, nb_loop_test,\n                       sigma2_catalog, sigma2_obs )\n\n# compute u0 to be in the attractor space\nu0    = [8.0;0.0;30.0]\ntspan = (0.0,5.0)\nprob  = ODEProblem(ssm.model, u0, tspan, parameters)\nu0    = last(solve(prob, reltol=1e-6, save_everystep=false))\n\nxt, yo, catalog = generate_data( ssm, u0 );","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"plot( xt.time, vcat(xt.values'...)[:,1])\nscatter!( yo.time, vcat(yo.values'...)[:,1]; markersize=2)","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"np = 100\ndata_assimilation = DataAssimilation( ssm, xt)\n@time xÃÇ = data_assimilation(yo, EnKs(np))\nRMSE(xt, xÃÇ)","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"plot(xt.time, vcat(xÃÇ.values'...)[:,1])\nscatter!(xt.time, vcat(xt.values'...)[:,1]; markersize=2)\nplot!(xt.time, vcat(xÃÇ.values'...)[:,2])\nscatter!(xt.time, vcat(xt.values'...)[:,2]; markersize=2)\nplot!(xt.time, vcat(xÃÇ.values'...)[:,3])\nscatter!(xt.time, vcat(xt.values'...)[:,3]; markersize=2)","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"p = plot3d(1, xlim=(-25,25), ylim=(-25,25), zlim=(0,50),\n            title = \"Lorenz 63\", marker = 2)\nfor x in eachrow(vcat(xÃÇ.values'...))\n    push!(p, x...)\nend\np","category":"page"}]
}
